#!/bin/bash

#SBATCH --partition=gpu
#SBATCH --gpus=1
#SBATCH --job-name=LED_6D
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16
#SBATCH --time=2:20:00
#SBATCH --output=4_6D_Dynamic_Experiments_output_%A.out


source activate led
module purge
module load 2022
module load CUDA/11.7.0

cd $HOME/MoTric/4_LED_Kitti_6D_Dynamic/
python main_led_nba.py --cfg 4_6D_Dynamic_Experiments --info 4_LED_Kitti_6D_Dynamic --dataset kitti


# Same parameters as in 2_X and 3. Most comparable to 3 which uses 3D translation only
# 6D: Using x y and z and rotation (Lie algebra)
# Evaluation: on translations only. So checking if with the current implementation, adding rotation information helps denoise translation predictions



# # ------------------- General Options -------------------------
# description                  : LED
# results_root_dir             : results
# dataset                      : kitti
# dimensions                   : 6 #with Lie rotation

# # ------------------- Dataset -------------------------
# past_frames                  : 15
# future_frames                : 24 #25 doesnt work for some reason
# min_past_frames              : 15
# min_future_frames            : 24

# k_preds                      : 24 #currently has to be the same as future... intended?

# # motion_dim                   : 3 #not used in code...
# # forecast_dim                 : 3 #not used in code...

# traj_mean                    : [14, 7.5] #middle of nba court
# traj_scale                   : 5

# # ------------------- Model -------------------------
# pretrained_core_denoising_model: './results/checkpoints/base_diffusion_model.p'
# debug                        : False # set to True for early stop in each epoch.

# diffusion                    : {
#   steps                      : 150, #50 more diffusion steps
#   beta_start                 : 1.e-4,
#   beta_end                   : 5.e-2,
#   beta_schedule              : 'linear'
# }

# # ------------------- Training Parameters -------------------------
# lr                           : 5.e-3 #tenth of previous
# train_batch_size             : 32
# test_batch_size              : 32 #slows testing down a bit compared to default 500
# num_epochs                   : 50
# test_interval                : 1  # Added: evaluate every N epochs
# lr_scheduler                 : 'step'
# decay_step                   : 8
# decay_gamma                  : 0.7 #slightly lower learning rate decay

