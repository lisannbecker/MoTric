#!/bin/bash

#SBATCH --partition=gpu
#SBATCH --gpus=1
#SBATCH --job-name=LED_Slower
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=18
#SBATCH --time=2:00:00
#SBATCH --output=2_1_Kitti_2D_LED_10In_5out_Dynamic_slower_learning_output_%A.out


source activate led
module purge
module load 2022
module load CUDA/11.7.0

cd $HOME/thesis/LED/
python main_led_nba.py --cfg 2_1_Kitti_2D_LED_10In_5out_Dynamic --info 2_1_Kitti_2D_LED_10In_5out_Dynamic_slower_learning --dataset kitti


# past_frames                  : 10
# future_frames                : 5
# traj_mean                    : [14, 7.5] <<removed for kitti
# num_epochs                   : 40
# train_batch_size             : 32
# test_batch_size              : 32
# steps                      : 150, #50 more diffusion steps
# lr                           : 5.e-3 #half of previous
# decay_gamma                  : 0.7 #slightly lower learning rate decay



# after seeing that making the model dynamic and removing checkpoints increased performance on kitti, test with new parameters